{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This process will pip install Kaggle and download data through Kaggle API.\n",
      "\n",
      "Please confirm that you've downloaded Kaggle JSON credentials into directory\n",
      "\n",
      "Requirement already satisfied: kaggle in /opt/anaconda3/lib/python3.8/site-packages (1.5.12)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.8/site-packages (from kaggle) (2021.10.8)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.8/site-packages (from kaggle) (2.27.1)\n",
      "Requirement already satisfied: python-slugify in /opt/anaconda3/lib/python3.8/site-packages (from kaggle) (5.0.2)\n",
      "Requirement already satisfied: python-dateutil in /opt/anaconda3/lib/python3.8/site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.8/site-packages (from kaggle) (4.62.3)\n",
      "Requirement already satisfied: six>=1.10 in /opt/anaconda3/lib/python3.8/site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: urllib3 in /opt/anaconda3/lib/python3.8/site-packages (from kaggle) (1.26.8)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /opt/anaconda3/lib/python3.8/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from requests->kaggle) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from requests->kaggle) (2.0.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing requirements for altair-saver: [Errno 2] No such file or directory: '/opt/anaconda3/lib/python3.8/site-packages/altair_saver-0.5.0.dist-info/METADATA'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401 - Unauthorized\n",
      "Data Successfully Downloaded\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from get_data import get_assets, get_positional_data\n",
    "\n",
    "get_assets()\n",
    "positions = get_positional_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def clean_positional(positions, first = 1, last = 17):\n",
    "    # reading plays (see play data https://www.kaggle.com/c/nfl-big-data-bowl-2021/data)\n",
    "    plays = pd.read_csv('nfl-big-data-bowl-2021/plays.csv')\n",
    "    games = pd.read_csv('nfl-big-data-bowl-2021/games.csv')\n",
    "    \n",
    "    #to_datetime\n",
    "    positions['time'] = pd.to_datetime(positions['time'], format='%Y-%m-%dT%H:%M:%S')\n",
    "    #print(positions.columns)\n",
    "\n",
    "    if (first != 1) or (last != 17):\n",
    "        week_game_id = list(games[games['week'].isin(list(np.arange(first,last+1)))]['gameId'].drop_duplicates())\n",
    "        positions = positions[positions['gameId'].isin(week_game_id)]\n",
    "\n",
    "    # Get starting position of offensive players\n",
    "    starting_pos = positions.groupby(['gameId', 'playId', 'position', 'nflId', 'team'])[['x', 'y']].first().reset_index()\n",
    "    \n",
    "    # merging play data (see play data https://www.kaggle.com/c/nfl-big-data-bowl-2021/data)\n",
    "    starting_pos_plays = starting_pos.merge(plays, on=['gameId', 'playId'], how='left')\n",
    "\n",
    "    # data cleaning where yardline is not Null\n",
    "    starting_pos_plays = starting_pos_plays[starting_pos_plays['absoluteYardlineNumber'].notnull()]\n",
    "\n",
    "    # bring in game info (see game info data https://www.kaggle.com/c/nfl-big-data-bowl-2021/data)\n",
    "    games = pd.read_csv('nfl-big-data-bowl-2021/games.csv')\n",
    "\n",
    "    #bringing in features from games\n",
    "    starting_pos_play_game = starting_pos_plays.merge(games, on='gameId', how='left')\n",
    "\n",
    "    #naming which team has the ball as offense or defense\n",
    "    starting_pos_play_game['offdef'] = np.where((starting_pos_play_game['team'] == 'away') &\n",
    "                                                (starting_pos_play_game['possessionTeam'] == starting_pos_play_game['visitorTeamAbbr']),\n",
    "                                                'offense', 'defense')\n",
    "\n",
    "    #starting position from offense players \n",
    "    starting_off = starting_pos_play_game[starting_pos_play_game['offdef'] == 'offense']\n",
    "\n",
    "    # What personal is on the field\n",
    "    personnel = starting_off['personnelO'].str.extract('(?P<RB>\\d+)\\sRB\\,\\s(?P<TE>\\d+)\\sTE\\,\\s(?P<WR>\\d+)\\sWR')\n",
    "    personnel = personnel.astype(float)\n",
    "\n",
    "    # Adding that as a feature in the new DF\n",
    "    starting_off_pers = pd.concat([starting_off, personnel], axis=1)\n",
    "\n",
    "    # Subtracting 10 because the endzone adds 10 years to field \n",
    "    starting_off_pers['yardline_100'] = starting_off_pers['absoluteYardlineNumber'].sub(10)\n",
    "\n",
    "    # If position X is less than yardline100, return yardline100 - starting position, else, starting position - yardline. \n",
    "    # This gets # of yards behind line no matter which way they are facing.\n",
    "\n",
    "    # Y starting is the y coords of the starting position.\n",
    "    starting_off_pers['off_pos'] = np.where(starting_off_pers['x'].lt(starting_off_pers['absoluteYardlineNumber']), 'left', 'right')\n",
    "    starting_off_pers['x_behind_line'] = np.where(starting_off_pers['off_pos'] == 'right',\n",
    "                                                starting_off_pers['absoluteYardlineNumber'].sub(starting_off_pers['x']),\n",
    "                                                starting_off_pers['x'].sub(starting_off_pers['absoluteYardlineNumber']))\n",
    "    starting_off_pers['y_starting'] = np.where(starting_off_pers['off_pos'] == 'right',\n",
    "                                            starting_off_pers['y'].rsub(53.3), starting_off_pers['y'])\n",
    "\n",
    "    # Y QB is the y starting position of the quarterback.\n",
    "    starting_off_pers['y_qb'] = starting_off_pers.groupby(['gameId', 'playId']).apply(lambda x: np.repeat(53.3/2, x.shape[0])\n",
    "        if x[x['position'] == 'QB'].shape[0] == 0 else np.repeat(x[x['position'] == 'QB']['y_starting'].iloc[0], x.shape[0])).explode().values\n",
    "    starting_off_pers['y_qb'] = starting_off_pers['y_qb'].astype(float)\n",
    "\n",
    "    # Find side of player relative to QB and the starting y coordinates relative to the QB.\n",
    "    starting_off_pers['qb_side'] = np.where(starting_off_pers['y_starting'].gt(starting_off_pers['y_qb']), 'R', 'L')\n",
    "    starting_off_pers['y_starting_qb'] = starting_off_pers['y_starting'].sub(starting_off_pers['y_qb'])\n",
    "\n",
    "    def find_rank(df, col, reverse=False):\n",
    "        \"\"\"\n",
    "        Find the ranking of a series based on values.\n",
    "        :param df: Dataframe for ranking; pd.DataFrame\n",
    "        :param col: Column from dataframe to rank; str\n",
    "        :param reverse: Flag of whether to reverse rank direction; bool\n",
    "        :return: Array with rankings; np.array\n",
    "        \"\"\"\n",
    "        # Extract series and use arsort to find rankings.\n",
    "        ser = df[col]\n",
    "        temp = np.argsort(ser)\n",
    "\n",
    "        # Reverse direction based on flag.\n",
    "        if reverse:\n",
    "            temp = temp[::-1]\n",
    "\n",
    "        # Fill ranking array.\n",
    "        ranks = np.empty_like(temp)\n",
    "        ranks[temp] = np.arange(ser.shape[0])\n",
    "        return ranks\n",
    "\n",
    "    # Find the order of positions based on offensive direction.\n",
    "    # First, group and extract first value of the y starting position and direction.\n",
    "    pos_start = (starting_off_pers\n",
    "                .groupby(['gameId', 'playId', 'position', 'nflId'])\n",
    "                [['y_starting', 'x', 'off_pos', 'qb_side']].first()\n",
    "                .reset_index())\n",
    "\n",
    "    # Next, group and extract ranking of positions based on whether team is home or away\n",
    "    # and the starting position.\n",
    "    pos_order = np.where(pos_start['position'] != 'QB',\n",
    "                         (pos_start.groupby(['gameId', 'playId', 'position', 'qb_side'])\n",
    "                          .apply(lambda x: np.where(x.index.get_level_values(-1) == 'R',\n",
    "                                                    find_rank(x, 'y_starting'),\n",
    "                                                    find_rank(x, 'y_starting', reverse=True)))\n",
    "                          .explode()\n",
    "                          .values\n",
    "                ),\n",
    "                         (pos_start.groupby(['gameId', 'playId', 'position'])\n",
    "                          .apply(lambda x: find_rank(x, 'y_starting'))\n",
    "                          .explode()\n",
    "                          .values\n",
    "                          )\n",
    "                         )\n",
    "\n",
    "    # Add column with the position order to the df with indexed starting position.\n",
    "    pos_start['pos_order'] = pos_order\n",
    "\n",
    "    # Add number of position to position label to get position number.\n",
    "    pos_start['pos_num'] = np.where(pos_start['position'] != 'QB',\n",
    "                                    pos_start['position'].add(pos_start['qb_side']).add(pos_start['pos_order'].astype(str)),\n",
    "                                    pos_start['position'].add(pos_start['pos_order'].astype(str)))\n",
    "\n",
    "    #Adding a label of the players position (WR1, WR2). This makes sense from a numerical stand point, but shouldn't be used\n",
    "    #to classify a team's WR1 WR2 etc.\n",
    "\n",
    "    starting_off_pers = starting_off_pers.merge(pos_start[['gameId', 'playId', 'nflId', 'pos_num', 'pos_order']],\n",
    "                                                on=['gameId', 'playId', 'nflId'])\n",
    "\n",
    "    # Convert to matrix of GameID and PlayID. Grab number of yards behind line for each player. \n",
    "    starting_x = (starting_off_pers\n",
    "        .pivot_table(columns='pos_num', index=['gameId', 'playId'], values='x_behind_line').rename(lambda x: x + '_x', axis=1))\n",
    "\n",
    "    #Same as above, but for Y coords relative to the QB.\n",
    "    starting_y = (starting_off_pers\n",
    "                .pivot_table(columns='pos_num', index=['gameId', 'playId'], values='y_starting_qb').rename(lambda x: x + '_y', axis=1))\n",
    "\n",
    "    #merging to get coords of players with _X and _Y\n",
    "    starting_pos = starting_x.merge(starting_y, left_index=True, right_index=True)\n",
    "\n",
    "    #X_col is getting all the X columns. Cols is creating a list that say \"WR1_in\", \"FB1_in\" etc\n",
    "    x_col = starting_pos.columns[starting_pos.columns.str.match('.*\\_x$')]\n",
    "    cols = [col[:4] + '_in' for col in x_col]\n",
    "\n",
    "    # Creating addition columns (boolean) for X player being in. If TE1 is in, flag says TRUE\n",
    "    starting_pos[cols] = starting_pos[x_col].notnull()\n",
    "    \n",
    "    #Sparse Matrix\n",
    "    starting_pos.fillna(0, inplace=True)\n",
    "\n",
    "    #Final data! Everything is getting merged together.\n",
    "    data = starting_pos.merge(starting_off_pers[['gameId', 'playId', 'offenseFormation']],\n",
    "                    left_index=True,\n",
    "                    right_on=['gameId', 'playId']).drop(['gameId', 'playId'], axis=1)\n",
    "\n",
    "    data.drop_duplicates(inplace=True)\n",
    "    data.dropna(axis=0, inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Will need to determine what gameIDs constitute our train/test sets. Probably just week 1-14 train 15-17 test. \n",
    "#  #train = clean_positional(positions, first = 1 , last = 14)\n",
    "#  #test = clean_positional(positions, first = 15, last = 17)\n",
    "\n",
    "data = clean_positional(positions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = clean_positional(positions, first = 1 , last = 14)\n",
    "test = clean_positional(positions, first = 15, last = 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "outputs": [
    {
     "data": {
      "text/plain": "       CBL0_x       CBR0_x  DBL0_x  DBR0_x       DEL0_x      DER0_x  \\\ncount  7600.0  7600.000000  7600.0  7600.0  7600.000000  7600.00000   \nmean      0.0    -0.000480     0.0     0.0    -0.000500    -0.00052   \nstd       0.0     0.041868     0.0     0.0     0.043589     0.04531   \nmin       0.0    -3.650000     0.0     0.0    -3.800000    -3.95000   \n25%       0.0     0.000000     0.0     0.0     0.000000     0.00000   \n50%       0.0     0.000000     0.0     0.0     0.000000     0.00000   \n75%       0.0     0.000000     0.0     0.0     0.000000     0.00000   \nmax       0.0     0.000000     0.0     0.0     0.000000     0.00000   \n\n            DLL0_x       DLR0_x  DTR0_x       FBL0_x  ...       TEL2_y  \\\ncount  7600.000000  7600.000000  7600.0  7600.000000  ...  7600.000000   \nmean     -0.002217    -0.001133     0.0    -0.066771  ...    -0.059276   \nstd       0.097021     0.069840     0.0     0.535573  ...     0.893787   \nmin      -4.510000    -4.370000     0.0    -5.500000  ...   -22.200000   \n25%       0.000000     0.000000     0.0     0.000000  ...     0.000000   \n50%       0.000000     0.000000     0.0     0.000000  ...     0.000000   \n75%       0.000000     0.000000     0.0     0.000000  ...     0.000000   \nmax       0.000000     0.000000     0.0     0.000000  ...     0.000000   \n\n            TER0_y       TER1_y       TER2_y       WRL0_y       WRL1_y  \\\ncount  7600.000000  7600.000000  7600.000000  7600.000000  7600.000000   \nmean      2.848811     0.210409     0.011655    -9.024015    -5.560556   \nstd       4.006602     1.138313     0.271118     6.171650     7.242647   \nmin       0.000000     0.000000     0.000000   -26.880000   -34.940000   \n25%       0.000000     0.000000     0.000000   -13.630000   -11.602500   \n50%       0.000000     0.000000     0.000000    -9.500000     0.000000   \n75%       5.172500     0.000000     0.000000    -4.162500     0.000000   \nmax      27.940000    20.970000     9.070000     0.000000     0.000000   \n\n            WRL2_y       WRR0_y       WRR1_y       WRR2_y  \ncount  7600.000000  7600.000000  7600.000000  7600.000000  \nmean     -1.138584    10.759373     4.902811     0.534191  \nstd       4.457694     6.858517     6.137226     2.067496  \nmin     -24.950000     0.000000     0.000000     0.000000  \n25%       0.000000     7.190000     0.000000     0.000000  \n50%       0.000000    11.852500     0.000000     0.000000  \n75%       0.000000    15.570000    10.470000     0.000000  \nmax       0.000000    26.830000    23.460000    22.410000  \n\n[8 rows x 116 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CBL0_x</th>\n      <th>CBR0_x</th>\n      <th>DBL0_x</th>\n      <th>DBR0_x</th>\n      <th>DEL0_x</th>\n      <th>DER0_x</th>\n      <th>DLL0_x</th>\n      <th>DLR0_x</th>\n      <th>DTR0_x</th>\n      <th>FBL0_x</th>\n      <th>...</th>\n      <th>TEL2_y</th>\n      <th>TER0_y</th>\n      <th>TER1_y</th>\n      <th>TER2_y</th>\n      <th>WRL0_y</th>\n      <th>WRL1_y</th>\n      <th>WRL2_y</th>\n      <th>WRR0_y</th>\n      <th>WRR1_y</th>\n      <th>WRR2_y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>7600.0</td>\n      <td>7600.000000</td>\n      <td>7600.0</td>\n      <td>7600.0</td>\n      <td>7600.000000</td>\n      <td>7600.00000</td>\n      <td>7600.000000</td>\n      <td>7600.000000</td>\n      <td>7600.0</td>\n      <td>7600.000000</td>\n      <td>...</td>\n      <td>7600.000000</td>\n      <td>7600.000000</td>\n      <td>7600.000000</td>\n      <td>7600.000000</td>\n      <td>7600.000000</td>\n      <td>7600.000000</td>\n      <td>7600.000000</td>\n      <td>7600.000000</td>\n      <td>7600.000000</td>\n      <td>7600.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.0</td>\n      <td>-0.000480</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.000500</td>\n      <td>-0.00052</td>\n      <td>-0.002217</td>\n      <td>-0.001133</td>\n      <td>0.0</td>\n      <td>-0.066771</td>\n      <td>...</td>\n      <td>-0.059276</td>\n      <td>2.848811</td>\n      <td>0.210409</td>\n      <td>0.011655</td>\n      <td>-9.024015</td>\n      <td>-5.560556</td>\n      <td>-1.138584</td>\n      <td>10.759373</td>\n      <td>4.902811</td>\n      <td>0.534191</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.0</td>\n      <td>0.041868</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.043589</td>\n      <td>0.04531</td>\n      <td>0.097021</td>\n      <td>0.069840</td>\n      <td>0.0</td>\n      <td>0.535573</td>\n      <td>...</td>\n      <td>0.893787</td>\n      <td>4.006602</td>\n      <td>1.138313</td>\n      <td>0.271118</td>\n      <td>6.171650</td>\n      <td>7.242647</td>\n      <td>4.457694</td>\n      <td>6.858517</td>\n      <td>6.137226</td>\n      <td>2.067496</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.0</td>\n      <td>-3.650000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-3.800000</td>\n      <td>-3.95000</td>\n      <td>-4.510000</td>\n      <td>-4.370000</td>\n      <td>0.0</td>\n      <td>-5.500000</td>\n      <td>...</td>\n      <td>-22.200000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-26.880000</td>\n      <td>-34.940000</td>\n      <td>-24.950000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-13.630000</td>\n      <td>-11.602500</td>\n      <td>0.000000</td>\n      <td>7.190000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-9.500000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>11.852500</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>5.172500</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-4.162500</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>15.570000</td>\n      <td>10.470000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>27.940000</td>\n      <td>20.970000</td>\n      <td>9.070000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>26.830000</td>\n      <td>23.460000</td>\n      <td>22.410000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows Ã— 116 columns</p>\n</div>"
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting missing columns as all zeroes for Test set\n",
    "for missing_col in set(train.columns).difference(test.columns):\n",
    "    test[missing_col] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train = train.drop('offenseFormation', axis = 1)\n",
    "y_train = train['offenseFormation']\n",
    "\n",
    "X_test = test.drop('offenseFormation', axis = 1)[X_train.columns]\n",
    "y_test = test['offenseFormation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.95986842, 0.94802632, 0.95065789, 0.96907895, 0.95263158])"
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=10000)\n",
    "cross_val_score(log_reg, X_train_scaled, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9547369097811546"
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params_lr = {'C': [10**x for x in range(-4, 4)]}\n",
    "grid_lr = GridSearchCV(log_reg, params_lr, cv=3, scoring='f1_micro')\n",
    "grid_lr.fit(X_train_scaled, y_train)\n",
    "grid_lr.best_score_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "outputs": [
    {
     "data": {
      "text/plain": "{'C': 10}"
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_lr.best_params_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.96184211, 0.94276316, 0.94671053, 0.97171053, 0.95855263])"
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfor = RandomForestClassifier()\n",
    "cross_val_score(rfor, X_train_scaled, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.9519736998585263"
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfor = RandomForestClassifier(random_state=0)\n",
    "params = {'max_depth': [None] + list(range(1, 5))}\n",
    "grid_rfor = GridSearchCV(rfor, param_grid=params, cv=3, scoring='f1_micro')\n",
    "grid_rfor.fit(X_train_scaled, y_train)\n",
    "grid_rfor.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'max_depth': None}"
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rfor.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.94802632, 0.93552632, 0.94802632, 0.96052632, 0.94605263])"
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "cross_val_score(dtree, X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.9422369785913549"
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtree = DecisionTreeClassifier(random_state=0)\n",
    "params = {'max_depth': [None] + list(range(1, 5)), 'min_samples_split': range(2, 10)}\n",
    "grid_dtree = GridSearchCV(dtree, param_grid=params, cv=3, scoring='f1_micro')\n",
    "grid_dtree.fit(X_train_scaled, y_train)\n",
    "grid_dtree.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       EMPTY       0.94      0.96      0.95       221\n",
      "      I_FORM       0.94      0.91      0.93        89\n",
      "       JUMBO       1.00      1.00      1.00         3\n",
      "      PISTOL       1.00      0.42      0.59        26\n",
      "     SHOTGUN       0.98      0.99      0.98      1072\n",
      "  SINGLEBACK       0.96      0.97      0.97       239\n",
      "\n",
      "    accuracy                           0.97      1650\n",
      "   macro avg       0.97      0.88      0.90      1650\n",
      "weighted avg       0.97      0.97      0.97      1650\n",
      "\n",
      "[[ 212    0    0    0    9    0]\n",
      " [   0   81    0    0    0    8]\n",
      " [   0    0    3    0    0    0]\n",
      " [   1    0    0   11   13    1]\n",
      " [  13    0    0    0 1059    0]\n",
      " [   0    5    0    0    1  233]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = grid_lr.predict(X_test_scaled)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       EMPTY       0.96      0.95      0.95       971\n",
      "      I_FORM       0.94      0.87      0.90       367\n",
      "       JUMBO       1.00      1.00      1.00        24\n",
      "      PISTOL       0.89      0.32      0.47        97\n",
      "     SHOTGUN       0.98      0.99      0.98      5037\n",
      "  SINGLEBACK       0.95      0.98      0.97      1099\n",
      "     WILDCAT       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           0.97      7600\n",
      "   macro avg       0.96      0.87      0.90      7600\n",
      "weighted avg       0.97      0.97      0.97      7600\n",
      "\n",
      "[[ 918    0    0    0   50    3    0]\n",
      " [   0  318    0    0    0   49    0]\n",
      " [   0    0   24    0    0    0    0]\n",
      " [   3    0    0   31   63    0    0]\n",
      " [  40    0    0    4 4993    0    0]\n",
      " [   0   21    0    0    0 1078    0]\n",
      " [   0    0    0    0    0    0    5]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = grid_lr.predict(X_train_scaled)\n",
    "print(classification_report(y_train, y_pred_train))\n",
    "print(confusion_matrix(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.6496969696969697"
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy = DummyClassifier(strategy='most_frequent')\n",
    "dummy.fit(X_train_scaled, y_train)\n",
    "y_pred_dummy = dummy.predict(X_test_scaled)\n",
    "np.mean(y_test == y_pred_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}