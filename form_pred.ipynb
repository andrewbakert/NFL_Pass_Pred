{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This process will pip install Kaggle and download data through Kaggle API.\n",
      "\n",
      "Please confirm that you've downloaded Kaggle JSON credentials into directory\n",
      "\n",
      "Requirement already satisfied: kaggle in /opt/anaconda3/lib/python3.8/site-packages (1.5.12)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.8/site-packages (from kaggle) (2021.10.8)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.8/site-packages (from kaggle) (2.27.1)\n",
      "Requirement already satisfied: python-slugify in /opt/anaconda3/lib/python3.8/site-packages (from kaggle) (5.0.2)\n",
      "Requirement already satisfied: python-dateutil in /opt/anaconda3/lib/python3.8/site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.8/site-packages (from kaggle) (4.62.3)\n",
      "Requirement already satisfied: six>=1.10 in /opt/anaconda3/lib/python3.8/site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: urllib3 in /opt/anaconda3/lib/python3.8/site-packages (from kaggle) (1.26.8)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /opt/anaconda3/lib/python3.8/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from requests->kaggle) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from requests->kaggle) (2.0.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing requirements for altair-saver: [Errno 2] No such file or directory: '/opt/anaconda3/lib/python3.8/site-packages/altair_saver-0.5.0.dist-info/METADATA'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401 - Unauthorized\n",
      "Data Successfully Downloaded\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from get_data import get_assets, get_positional_data\n",
    "\n",
    "get_assets()\n",
    "positions = get_positional_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def clean_positional(positions, first = 1, last = 17):\n",
    "    # reading plays (see play data https://www.kaggle.com/c/nfl-big-data-bowl-2021/data)\n",
    "    plays = pd.read_csv('nfl-big-data-bowl-2021/plays.csv')\n",
    "    games = pd.read_csv('nfl-big-data-bowl-2021/games.csv')\n",
    "    \n",
    "    #to_datetime\n",
    "    positions['time'] = pd.to_datetime(positions['time'], format='%Y-%m-%dT%H:%M:%S')\n",
    "    #print(positions.columns)\n",
    "\n",
    "    if (first != 1) or (last != 17):\n",
    "        week_game_id = list(games[games['week'].isin(np.arange(first,last+1))]['gameId'].drop_duplicates())\n",
    "        positions = positions[positions['gameId'].isin(week_game_id)]\n",
    "\n",
    "    # Get starting position of offensive players\n",
    "    starting_pos = positions.groupby(['gameId', 'playId', 'position', 'nflId', 'team'])[['x', 'y']].first().reset_index()\n",
    "    \n",
    "    # merging play data (see play data https://www.kaggle.com/c/nfl-big-data-bowl-2021/data)\n",
    "    starting_pos_plays = starting_pos.merge(plays, on=['gameId', 'playId'], how='left')\n",
    "\n",
    "    # data cleaning where yardline is not Null\n",
    "    starting_pos_plays = starting_pos_plays[starting_pos_plays['absoluteYardlineNumber'].notnull()]\n",
    "\n",
    "    # bring in game info (see game info data https://www.kaggle.com/c/nfl-big-data-bowl-2021/data)\n",
    "    games = pd.read_csv('nfl-big-data-bowl-2021/games.csv')\n",
    "\n",
    "    #bringing in features from games\n",
    "    starting_pos_play_game = starting_pos_plays.merge(games, on='gameId', how='left')\n",
    "\n",
    "    #naming which team has the ball as offense or defense\n",
    "    starting_pos_play_game['offdef'] = np.where((starting_pos_play_game['team'] == 'away') &\n",
    "                                                (starting_pos_play_game['possessionTeam'] == starting_pos_play_game['visitorTeamAbbr']),\n",
    "                                                'offense', 'defense')\n",
    "\n",
    "    #starting position from offense players \n",
    "    starting_off = starting_pos_play_game[starting_pos_play_game['offdef'] == 'offense']\n",
    "\n",
    "    # What personal is on the field\n",
    "    personnel = starting_off['personnelO'].str.extract('(?P<RB>\\d+)\\sRB\\,\\s(?P<TE>\\d+)\\sTE\\,\\s(?P<WR>\\d+)\\sWR')\n",
    "    personnel = personnel.astype(float)\n",
    "\n",
    "    # Adding that as a feature in the new DF\n",
    "    starting_off_pers = pd.concat([starting_off, personnel], axis=1)\n",
    "\n",
    "    # Subtracting 10 because the endzone adds 10 years to field \n",
    "    starting_off_pers['yardline_100'] = starting_off_pers['absoluteYardlineNumber'].sub(10)\n",
    "\n",
    "    # If position X is less than yardline100, return yardline100 - starting position, else, starting position - yardline. \n",
    "    # This gets # of yards behind line no matter which way they are facing.\n",
    "\n",
    "    # Y starting is the y coords of the starting position.\n",
    "    starting_off_pers['off_pos'] = np.where(starting_off_pers['x'].lt(starting_off_pers['absoluteYardlineNumber']), 'left', 'right')\n",
    "    starting_off_pers['x_behind_line'] = np.where(starting_off_pers['off_pos'] == 'right',\n",
    "                                                starting_off_pers['absoluteYardlineNumber'].sub(starting_off_pers['x']),\n",
    "                                                starting_off_pers['x'].sub(starting_off_pers['absoluteYardlineNumber']))\n",
    "    starting_off_pers['y_starting'] = np.where(starting_off_pers['off_pos'] == 'right',\n",
    "                                            starting_off_pers['y'].rsub(53.3), starting_off_pers['y'])\n",
    "\n",
    "    # Y QB is the y starting position of the quarterback.\n",
    "    starting_off_pers['y_qb'] = starting_off_pers.groupby(['gameId', 'playId']).apply(lambda x: np.repeat(53.3/2, x.shape[0])\n",
    "        if x[x['position'] == 'QB'].shape[0] == 0 else np.repeat(x[x['position'] == 'QB']['y_starting'].iloc[0], x.shape[0])).explode().values\n",
    "    starting_off_pers['y_qb'] = starting_off_pers['y_qb'].astype(float)\n",
    "\n",
    "    # Find side of player relative to QB and the starting y coordinates relative to the QB.\n",
    "    starting_off_pers['qb_side'] = np.where(starting_off_pers['y_starting'].gt(starting_off_pers['y_qb']), 'R', 'L')\n",
    "    starting_off_pers['y_starting_qb'] = starting_off_pers['y_starting'].sub(starting_off_pers['y_qb'])\n",
    "\n",
    "    def find_rank(df, col, reverse=False):\n",
    "        \"\"\"\n",
    "        Find the ranking of a series based on values.\n",
    "        :param df: Dataframe for ranking; pd.DataFrame\n",
    "        :param col: Column from dataframe to rank; str\n",
    "        :param reverse: Flag of whether to reverse rank direction; bool\n",
    "        :return: Array with rankings; np.array\n",
    "        \"\"\"\n",
    "        # Extract series and use arsort to find rankings.\n",
    "        ser = df[col]\n",
    "        temp = np.argsort(ser)\n",
    "\n",
    "        # Reverse direction based on flag.\n",
    "        if reverse:\n",
    "            temp = temp[::-1]\n",
    "\n",
    "        # Fill ranking array.\n",
    "        ranks = np.empty_like(temp)\n",
    "        ranks[temp] = np.arange(ser.shape[0])\n",
    "        return ranks\n",
    "\n",
    "    # Find the order of positions based on offensive direction.\n",
    "    # First, group and extract first value of the y starting position and direction.\n",
    "    pos_start = (starting_off_pers\n",
    "                .groupby(['gameId', 'playId', 'position', 'nflId'])\n",
    "                [['y_starting', 'x', 'off_pos', 'qb_side']].first()\n",
    "                .reset_index())\n",
    "\n",
    "    # Next, group and extract ranking of positions based on whether team is home or away\n",
    "    # and the starting position.\n",
    "    pos_order = np.where(pos_start['position'] != 'QB',\n",
    "                         (pos_start.groupby(['gameId', 'playId', 'position', 'qb_side'])\n",
    "                          .apply(lambda x: np.where(x.index.get_level_values(-1) == 'R',\n",
    "                                                    find_rank(x, 'y_starting'),\n",
    "                                                    find_rank(x, 'y_starting', reverse=True)))\n",
    "                          .explode()\n",
    "                          .values\n",
    "                ),\n",
    "                         (pos_start.groupby(['gameId', 'playId', 'position'])\n",
    "                          .apply(lambda x: find_rank(x, 'y_starting'))\n",
    "                          .explode()\n",
    "                          .values\n",
    "                          )\n",
    "                         )\n",
    "\n",
    "    # Add column with the position order to the df with indexed starting position.\n",
    "    pos_start['pos_order'] = pos_order\n",
    "\n",
    "    # Add number of position to position label to get position number.\n",
    "    pos_start['pos_num'] = np.where(pos_start['position'] != 'QB',\n",
    "                                    pos_start['position'].add(pos_start['qb_side']).add(pos_start['pos_order'].astype(str)),\n",
    "                                    pos_start['position'].add(pos_start['pos_order'].astype(str)))\n",
    "\n",
    "    #Adding a label of the players position (WR1, WR2). This makes sense from a numerical stand point, but shouldn't be used\n",
    "    #to classify a team's WR1 WR2 etc.\n",
    "\n",
    "    starting_off_pers = starting_off_pers.merge(pos_start[['gameId', 'playId', 'nflId', 'pos_num', 'pos_order']],\n",
    "                                                on=['gameId', 'playId', 'nflId'])\n",
    "\n",
    "    # Convert to matrix of GameID and PlayID. Grab number of yards behind line for each player. \n",
    "    starting_x = (starting_off_pers\n",
    "        .pivot_table(columns='pos_num', index=['gameId', 'playId'], values='x_behind_line').rename(lambda x: x + '_x', axis=1))\n",
    "\n",
    "    #Same as above, but for Y coords relative to the QB.\n",
    "    starting_y = (starting_off_pers\n",
    "                .pivot_table(columns='pos_num', index=['gameId', 'playId'], values='y_starting_qb').rename(lambda x: x + '_y', axis=1))\n",
    "\n",
    "    #merging to get coords of players with _X and _Y\n",
    "    starting_pos = starting_x.merge(starting_y, left_index=True, right_index=True)\n",
    "\n",
    "    #X_col is getting all the X columns. Cols is creating a list that say \"WR1_in\", \"FB1_in\" etc\n",
    "    x_col = starting_pos.columns[starting_pos.columns.str.match('.*\\_x$')]\n",
    "    cols = [col[:4] + '_in' for col in x_col]\n",
    "\n",
    "    # Creating addition columns (boolean) for X player being in. If TE1 is in, flag says TRUE\n",
    "    starting_pos[cols] = starting_pos[x_col].notnull()\n",
    "\n",
    "    #Sparse Matrix\n",
    "    starting_pos.fillna(0, inplace=True)\n",
    "\n",
    "    #Final data! Everything is getting merged together.\n",
    "    data = starting_pos.merge(starting_off_pers[['gameId', 'playId', 'offenseFormation']].drop_duplicates(),\n",
    "                    left_index=True,\n",
    "                    right_on=['gameId', 'playId']).drop(['gameId', 'playId'], axis=1)\n",
    "\n",
    "    data.dropna(axis=0, inplace=True)\n",
    "    data = data.loc[:, ~np.all(data == 0, axis=0)]\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Will need to determine what gameIDs constitute our train/test sets. Probably just week 1-14 train 15-17 test. \n",
    "#  #train = clean_positional(positions, first = 1 , last = 14)\n",
    "#  #test = clean_positional(positions, first = 15, last = 17)\n",
    "\n",
    "data = clean_positional(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = clean_positional(positions, first = 1 , last = 14)\n",
    "test = clean_positional(positions, first = 15, last = 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting missing columns as all zeroes for Test set\n",
    "for missing_col in set(train.columns).difference(test.columns):\n",
    "    test[missing_col] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train = train.drop('offenseFormation', axis = 1)\n",
    "y_train = train['offenseFormation']\n",
    "\n",
    "X_test = test.drop('offenseFormation', axis = 1)[X_train.columns]\n",
    "y_test = test['offenseFormation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.95986842, 0.94802632, 0.95065789, 0.96907895, 0.95263158])"
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=10000)\n",
    "cross_val_score(log_reg, X_train_scaled, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9547369097811546"
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params_lr = {'C': [10**x for x in range(-4, 4)]}\n",
    "grid_lr = GridSearchCV(log_reg, params_lr, cv=3, scoring='f1_micro')\n",
    "grid_lr.fit(X_train_scaled, y_train)\n",
    "grid_lr.best_score_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "outputs": [
    {
     "data": {
      "text/plain": "{'C': 10}"
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_lr.best_params_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.96184211, 0.94473684, 0.95460526, 0.97434211, 0.96052632])"
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfor = RandomForestClassifier()\n",
    "cross_val_score(rfor, X_train_scaled, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.9584213974068992"
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfor = RandomForestClassifier(random_state=0)\n",
    "params = {'max_depth': [None] + list(range(1, 5))}\n",
    "grid_rfor = GridSearchCV(rfor, param_grid=params, cv=3, scoring='f1_micro')\n",
    "grid_rfor.fit(X_train_scaled, y_train)\n",
    "grid_rfor.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'max_depth': None}"
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rfor.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.94605263, 0.93618421, 0.94473684, 0.9625    , 0.94605263])"
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "cross_val_score(dtree, X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.9426317154471265"
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtree = DecisionTreeClassifier(random_state=0)\n",
    "params = {'max_depth': [None] + list(range(1, 5)), 'min_samples_split': range(2, 10)}\n",
    "grid_dtree = GridSearchCV(dtree, param_grid=params, cv=3, scoring='f1_micro')\n",
    "grid_dtree.fit(X_train_scaled, y_train)\n",
    "grid_dtree.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       EMPTY       0.94      0.96      0.95       221\n",
      "      I_FORM       0.94      0.91      0.93        89\n",
      "       JUMBO       1.00      1.00      1.00         3\n",
      "      PISTOL       1.00      0.42      0.59        26\n",
      "     SHOTGUN       0.98      0.99      0.98      1072\n",
      "  SINGLEBACK       0.96      0.97      0.97       239\n",
      "\n",
      "    accuracy                           0.97      1650\n",
      "   macro avg       0.97      0.88      0.90      1650\n",
      "weighted avg       0.97      0.97      0.97      1650\n",
      "\n",
      "[[ 212    0    0    0    9    0]\n",
      " [   0   81    0    0    0    8]\n",
      " [   0    0    3    0    0    0]\n",
      " [   1    0    0   11   13    1]\n",
      " [  13    0    0    0 1059    0]\n",
      " [   0    5    0    0    1  233]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = grid_lr.predict(X_test_scaled)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       EMPTY       0.96      0.95      0.95       971\n",
      "      I_FORM       0.94      0.87      0.90       367\n",
      "       JUMBO       1.00      1.00      1.00        24\n",
      "      PISTOL       0.89      0.33      0.48        97\n",
      "     SHOTGUN       0.98      0.99      0.98      5037\n",
      "  SINGLEBACK       0.95      0.98      0.97      1099\n",
      "     WILDCAT       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           0.97      7600\n",
      "   macro avg       0.96      0.87      0.90      7600\n",
      "weighted avg       0.97      0.97      0.97      7600\n",
      "\n",
      "[[ 918    0    0    0   50    3    0]\n",
      " [   0  318    0    0    0   49    0]\n",
      " [   0    0   24    0    0    0    0]\n",
      " [   3    0    0   32   62    0    0]\n",
      " [  40    0    0    4 4993    0    0]\n",
      " [   0   21    0    0    0 1078    0]\n",
      " [   0    0    0    0    0    0    5]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = grid_lr.predict(X_train_scaled)\n",
    "print(classification_report(y_train, y_pred_train))\n",
    "print(confusion_matrix(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.6496969696969697"
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy = DummyClassifier(strategy='most_frequent')\n",
    "dummy.fit(X_train_scaled, y_train)\n",
    "y_pred_dummy = dummy.predict(X_test_scaled)\n",
    "np.mean(y_test == y_pred_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}